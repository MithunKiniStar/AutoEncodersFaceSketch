{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational Autoencoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzFek38gjGv1BXX6/EXGoL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MithunKiniStar/AutoEncodersFaceSketch/blob/main/Variational_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.2.4\n",
        "!pip install tensorflow==1.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3zvShavj_j-",
        "outputId": "aea0922d-01ab-4dfd-9bce-707945589a75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.21.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.4) (1.5.2)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 2.2.4 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.2.4 keras-applications-1.0.8\n",
            "Collecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 70 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.5.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 66.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.43.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.21.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.1)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 50.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\n",
            "Successfully installed mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Conv2DTranspose, Flatten\n",
        "from tensorflow.keras.layers import Reshape, BatchNormalization, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sample_z(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "    mu, sigma = args\n",
        "    batch     = K.shape(mu)[0]\n",
        "    dim       = K.int_shape(mu)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    eps       = K.random_normal(shape=(batch, dim))\n",
        "    return mu + K.exp(sigma / 2) * eps"
      ],
      "metadata": {
        "id": "2NBxlp38tk_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_width  = 256\n",
        "image_height = 256\n",
        "latent_dim   = 2\n",
        "no_epochs    = 30\n",
        "batch_size   = 128\n",
        "num_channels = 3"
      ],
      "metadata": {
        "id": "m32acXUwt2VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QymTiGk4epvb",
        "outputId": "0310447c-23cd-4fce-a6e4-bfeb82a11c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 128, 128, 32) 896         encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 128, 128, 32) 128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 64, 64, 64)   18496       batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 64, 64, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 262144)       0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 16)           4194320     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 16)           64          dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "latent_mu (Dense)               (None, 2)            34          batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "latent_sigma (Dense)            (None, 2)            34          batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 2)            0           latent_mu[0][0]                  \n",
            "                                                                 latent_sigma[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 4,214,228\n",
            "Trainable params: 4,214,004\n",
            "Non-trainable params: 224\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Defining the encoder\n",
        "inputs = Input(shape=(image_width, image_height, 3), name='encoder_input')  \n",
        "\n",
        "x = Conv2D(32, (3, 3), activation='relu', strides=2, padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "conv_shape = K.int_shape(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(16)(x)\n",
        "x = BatchNormalization()(x)\n",
        "mu = Dense(latent_dim, name='latent_mu')(x)\n",
        "sigma = Dense(latent_dim, name='latent_sigma')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "z  = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])\n",
        "\n",
        "\n",
        "encoder = Model(inputs, [mu, sigma, z], name='encoder')\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining decoder\n",
        "d_i   = Input(shape=(latent_dim, ), name='decoder_input')\n",
        "x     = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(d_i)\n",
        "x     = BatchNormalization()(x)\n",
        "x     = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
        "cx    = Conv2DTranspose(16, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "cx    = BatchNormalization()(cx)\n",
        "cx    = Conv2DTranspose(8, (3, 3), strides=2, padding='same',  activation='relu')(cx)\n",
        "cx    = BatchNormalization()(cx)\n",
        "o     = Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same', name='decoder_output')(cx)\n",
        "\n",
        "# Instantiate decoder\n",
        "decoder = Model(d_i, o, name='decoder')\n",
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq6xXT4EjQXl",
        "outputId": "56c7345b-c6cd-4a66-915c-0092167e0209"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 262144)            786432    \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_3 (Ba (None, 262144)            1048576   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 128, 128, 16)      9232      \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_4 (Ba (None, 128, 128, 16)      64        \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 256, 256, 8)       1160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_5 (Ba (None, 256, 256, 8)       32        \n",
            "_________________________________________________________________\n",
            "decoder_output (Conv2DTransp (None, 256, 256, 3)       219       \n",
            "=================================================================\n",
            "Total params: 1,845,715\n",
            "Trainable params: 1,321,379\n",
            "Non-trainable params: 524,336\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae')\n",
        "vae.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vStF6Bn5jTeR",
        "outputId": "cd2523ff-03c4-4c35-9df5-c2bd3d38c6f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              [(None, 2), (None, 2), (N 4214228   \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 256, 256, 3)       1845715   \n",
            "=================================================================\n",
            "Total params: 6,059,943\n",
            "Trainable params: 5,535,383\n",
            "Non-trainable params: 524,560\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(true, pred):\n",
        "    # Reconstruction loss\n",
        "    reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * image_width * image_height\n",
        "    # KL divergence loss\n",
        "    kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
        "    kl_loss = K.sum(kl_loss, axis=-1)\n",
        "    kl_loss *= -0.5\n",
        "    # Total loss = 50% rec + 50% KL divergence loss\n",
        "    return K.mean(reconstruction_loss + kl_loss)"
      ],
      "metadata": {
        "id": "vkkOBivwjWSO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Dropout, Input\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm \n",
        "import os\n",
        "import re \n",
        "# to get the files in proper order\n",
        "def sorted_alphanumeric(data):  \n",
        "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n",
        "    return sorted(data,key = alphanum_key)\n",
        "\n",
        "\n",
        "# defining the size of image \n",
        "SIZE = 256\n",
        "\n",
        "image_path = '/content/photos'\n",
        "img_array = []\n",
        "\n",
        "sketch_path = '/content/sketches'\n",
        "sketch_array = []\n",
        "\n",
        "image_file = sorted_alphanumeric(os.listdir(image_path))\n",
        "sketch_file = sorted_alphanumeric(os.listdir(sketch_path))\n",
        "\n",
        "\n",
        "for i in tqdm(image_file):\n",
        "    image = cv2.imread(image_path + '/' + i,1)\n",
        "    print(\"Image is \",image)\n",
        "    \n",
        "    # as opencv load image in bgr format converting it to rgb\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # resizing images \n",
        "    image = cv2.resize(image, (SIZE, SIZE))\n",
        "    \n",
        "    # normalizing image \n",
        "    image = image.astype('float32') / 255.0\n",
        "    \n",
        "    #appending normal normal image    \n",
        "    img_array.append(img_to_array(image))\n",
        "    # Image Augmentation\n",
        "    \n",
        "    # horizontal flip \n",
        "    img1 = cv2.flip(image,1)\n",
        "    img_array.append(img_to_array(img1))\n",
        "     #vertical flip \n",
        "    img2 = cv2.flip(image,-1)\n",
        "    img_array.append(img_to_array(img2))\n",
        "     #vertical flip \n",
        "    img3 = cv2.flip(image,-1)\n",
        "    # horizontal flip\n",
        "    img3 = cv2.flip(img3,1)\n",
        "    img_array.append(img_to_array(img3))\n",
        "    # rotate clockwise \n",
        "    img4 = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
        "    img_array.append(img_to_array(img4))\n",
        "    # flip rotated image \n",
        "    img5 = cv2.flip(img4,1)\n",
        "    img_array.append(img_to_array(img5))\n",
        "     # rotate anti clockwise \n",
        "    img6 = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "    img_array.append(img_to_array(img6))\n",
        "    # flip rotated image \n",
        "    img7 = cv2.flip(img6,1)\n",
        "    img_array.append(img_to_array(img7))\n",
        "  \n",
        "    \n",
        "for i in tqdm(sketch_file):\n",
        "    image = cv2.imread(sketch_path + '/' + i,1)\n",
        "    \n",
        "    # as opencv load image in bgr format converting it to rgb\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # resizing images \n",
        "    image = cv2.resize(image, (SIZE, SIZE))\n",
        "    \n",
        "    # normalizing image \n",
        "    image = image.astype('float32') / 255.0\n",
        "    # appending normal sketch image\n",
        "    sketch_array.append(img_to_array(image))\n",
        "    \n",
        "    #Image Augmentation\n",
        "    # horizontal flip \n",
        "    img1 = cv2.flip(image,1)\n",
        "    sketch_array.append(img_to_array(img1))\n",
        "     #vertical flip \n",
        "    img2 = cv2.flip(image,-1)\n",
        "    sketch_array.append(img_to_array(img2))\n",
        "     #vertical flip \n",
        "    img3 = cv2.flip(image,-1)\n",
        "    # horizontal flip\n",
        "    img3 = cv2.flip(img3,1)\n",
        "    sketch_array.append(img_to_array(img3))\n",
        "    # rotate clockwise \n",
        "    img4 = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
        "    sketch_array.append(img_to_array(img4))\n",
        "    # flip rotated image \n",
        "    img5 = cv2.flip(img4,1)\n",
        "    sketch_array.append(img_to_array(img5))\n",
        "     # rotate anti clockwise \n",
        "    img6 = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "    sketch_array.append(img_to_array(img6))\n",
        "    # flip rotated image \n",
        "    img7 = cv2.flip(img6,1)\n",
        "    sketch_array.append(img_to_array(img7))"
      ],
      "metadata": {
        "id": "FDf3KNt3jaNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of sketch images:\",len(sketch_array))\n",
        "print(\"Total number of images:\",len(img_array))\n",
        "train_sketch_image = sketch_array[:1400]\n",
        "train_image = img_array[:1400]\n",
        "test_sketch_image = sketch_array[1400:]\n",
        "test_image = img_array[1400:]\n",
        "# reshaping\n",
        "train_sketch_image = np.reshape(train_sketch_image,(len(train_sketch_image),SIZE,SIZE,3))\n",
        "train_image = np.reshape(train_image, (len(train_image),SIZE,SIZE,3))\n",
        "print('Train color image shape:',train_image.shape)\n",
        "test_sketch_image = np.reshape(test_sketch_image,(len(test_sketch_image),SIZE,SIZE,3))\n",
        "test_image = np.reshape(test_image, (len(test_image),SIZE,SIZE,3))\n",
        "print('Test color image shape',test_image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LKP0GP6je4-",
        "outputId": "8571a370-f67c-4e69-90c0-2013c2148880"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sketch images: 1504\n",
            "Total number of images: 1504\n",
            "Train color image shape: (1400, 256, 256, 3)\n",
            "Test color image shape (104, 256, 256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelling for Face To Sketch**"
      ],
      "metadata": {
        "id": "eHi4GDl93o6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae.compile(optimizer='adam', loss=vae_loss)\n",
        "vae.fit(train_image, train_sketch_image,\n",
        "        epochs=100,\n",
        "        batch_size=80,\n",
        "        validation_data=(test_image, test_sketch_image))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVAsCO84jfjc",
        "outputId": "3b8266e1-8bbc-44b2-fc8e-7eae774aa07f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1400 samples, validate on 104 samples\n",
            "Epoch 1/100\n",
            "1400/1400 [==============================] - 321s 229ms/sample - loss: 42204.0670 - val_loss: 42156.6782\n",
            "Epoch 2/100\n",
            "1400/1400 [==============================] - 318s 227ms/sample - loss: 38566.9859 - val_loss: 39718.6379\n",
            "Epoch 3/100\n",
            "1400/1400 [==============================] - 318s 227ms/sample - loss: 35814.8190 - val_loss: 36613.8380\n",
            "Epoch 4/100\n",
            "1400/1400 [==============================] - 320s 228ms/sample - loss: 32844.1520 - val_loss: 33679.0571\n",
            "Epoch 5/100\n",
            "1400/1400 [==============================] - 319s 228ms/sample - loss: 30025.0710 - val_loss: 31697.5255\n",
            "Epoch 6/100\n",
            "1400/1400 [==============================] - 318s 227ms/sample - loss: 27852.1708 - val_loss: 30816.6858\n",
            "Epoch 7/100\n",
            "1400/1400 [==============================] - 318s 227ms/sample - loss: 26606.1908 - val_loss: 30671.3239\n",
            "Epoch 8/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 25897.9902 - val_loss: 31001.0859\n",
            "Epoch 9/100\n",
            "1400/1400 [==============================] - 316s 225ms/sample - loss: 25557.4653 - val_loss: 31058.7025\n",
            "Epoch 10/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 25368.7896 - val_loss: 31267.0608\n",
            "Epoch 11/100\n",
            "1400/1400 [==============================] - 314s 224ms/sample - loss: 25277.9921 - val_loss: 31220.6125\n",
            "Epoch 12/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 25101.9074 - val_loss: 31253.9237\n",
            "Epoch 13/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 24935.3588 - val_loss: 31125.3642\n",
            "Epoch 14/100\n",
            "1400/1400 [==============================] - 316s 225ms/sample - loss: 24857.5143 - val_loss: 30703.0512\n",
            "Epoch 15/100\n",
            "1400/1400 [==============================] - 313s 224ms/sample - loss: 24864.8670 - val_loss: 30522.9112\n",
            "Epoch 16/100\n",
            "1400/1400 [==============================] - 313s 223ms/sample - loss: 24763.3012 - val_loss: 29955.2172\n",
            "Epoch 17/100\n",
            "1400/1400 [==============================] - 310s 222ms/sample - loss: 24696.9879 - val_loss: 29539.9419\n",
            "Epoch 18/100\n",
            "1400/1400 [==============================] - 311s 222ms/sample - loss: 24637.3025 - val_loss: 29454.1588\n",
            "Epoch 19/100\n",
            "1400/1400 [==============================] - 312s 223ms/sample - loss: 24804.9493 - val_loss: 27932.1627\n",
            "Epoch 20/100\n",
            "1400/1400 [==============================] - 311s 222ms/sample - loss: 24595.5827 - val_loss: 26993.1181\n",
            "Epoch 21/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 24472.0756 - val_loss: 26686.4979\n",
            "Epoch 22/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 24547.3595 - val_loss: 26572.7192\n",
            "Epoch 23/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 24516.7001 - val_loss: 25791.9752\n",
            "Epoch 24/100\n",
            "1400/1400 [==============================] - 314s 224ms/sample - loss: 24443.7907 - val_loss: 25521.2172\n",
            "Epoch 25/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 24403.0528 - val_loss: 25020.7441\n",
            "Epoch 26/100\n",
            "1400/1400 [==============================] - 313s 223ms/sample - loss: 24388.9868 - val_loss: 24759.3072\n",
            "Epoch 27/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 24402.9564 - val_loss: 24415.7016\n",
            "Epoch 28/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 24327.8789 - val_loss: 24260.8105\n",
            "Epoch 29/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 24239.2138 - val_loss: 24019.4890\n",
            "Epoch 30/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 24149.8402 - val_loss: 23858.6363\n",
            "Epoch 31/100\n",
            "1400/1400 [==============================] - 314s 224ms/sample - loss: 24502.2626 - val_loss: 23871.8484\n",
            "Epoch 32/100\n",
            "1400/1400 [==============================] - 314s 224ms/sample - loss: 24169.7536 - val_loss: 23765.0272\n",
            "Epoch 33/100\n",
            "1400/1400 [==============================] - 314s 225ms/sample - loss: 24081.4096 - val_loss: 23866.5401\n",
            "Epoch 34/100\n",
            "1400/1400 [==============================] - 314s 225ms/sample - loss: 24056.4261 - val_loss: 23573.0500\n",
            "Epoch 35/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 24122.8150 - val_loss: 23776.1699\n",
            "Epoch 36/100\n",
            "1400/1400 [==============================] - 316s 225ms/sample - loss: 23954.9710 - val_loss: 23489.4730\n",
            "Epoch 37/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23934.0016 - val_loss: 23715.6855\n",
            "Epoch 38/100\n",
            "1400/1400 [==============================] - 317s 227ms/sample - loss: 23966.5394 - val_loss: 23504.0032\n",
            "Epoch 39/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23910.1276 - val_loss: 23730.8505\n",
            "Epoch 40/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 23941.6610 - val_loss: 23804.5957\n",
            "Epoch 41/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23817.9011 - val_loss: 23732.8983\n",
            "Epoch 42/100\n",
            "1400/1400 [==============================] - 316s 225ms/sample - loss: 23872.7747 - val_loss: 23764.1140\n",
            "Epoch 43/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 23883.7738 - val_loss: 23525.4587\n",
            "Epoch 44/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 23805.3767 - val_loss: 23622.5936\n",
            "Epoch 45/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 23717.5968 - val_loss: 23522.6908\n",
            "Epoch 46/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23694.3488 - val_loss: 23719.3597\n",
            "Epoch 47/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23636.7046 - val_loss: 23639.0168\n",
            "Epoch 48/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23732.5010 - val_loss: 23728.9141\n",
            "Epoch 49/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23691.8720 - val_loss: 23792.7121\n",
            "Epoch 50/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23614.1641 - val_loss: 23666.3724\n",
            "Epoch 51/100\n",
            "1400/1400 [==============================] - 316s 225ms/sample - loss: 23579.8864 - val_loss: 23655.5216\n",
            "Epoch 52/100\n",
            "1400/1400 [==============================] - 314s 224ms/sample - loss: 23619.1529 - val_loss: 24273.9046\n",
            "Epoch 53/100\n",
            "1400/1400 [==============================] - 313s 224ms/sample - loss: 23554.2288 - val_loss: 23706.3169\n",
            "Epoch 54/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23621.7185 - val_loss: 23690.9662\n",
            "Epoch 55/100\n",
            "1400/1400 [==============================] - 314s 224ms/sample - loss: 23482.0788 - val_loss: 23796.9361\n",
            "Epoch 56/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 23546.7522 - val_loss: 23775.9184\n",
            "Epoch 57/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23463.9310 - val_loss: 23726.7673\n",
            "Epoch 58/100\n",
            "1400/1400 [==============================] - 316s 225ms/sample - loss: 23457.5967 - val_loss: 23864.5440\n",
            "Epoch 59/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 23442.8260 - val_loss: 23870.7769\n",
            "Epoch 60/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23388.0721 - val_loss: 24080.4330\n",
            "Epoch 61/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23281.2910 - val_loss: 23939.3274\n",
            "Epoch 62/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23380.8055 - val_loss: 23986.2903\n",
            "Epoch 63/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23178.3791 - val_loss: 23759.3517\n",
            "Epoch 64/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23186.4931 - val_loss: 23867.5623\n",
            "Epoch 65/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23298.5272 - val_loss: 23751.0888\n",
            "Epoch 66/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23348.1076 - val_loss: 24381.7523\n",
            "Epoch 67/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23220.3982 - val_loss: 23802.9794\n",
            "Epoch 68/100\n",
            "1400/1400 [==============================] - 314s 224ms/sample - loss: 23144.2859 - val_loss: 23921.2009\n",
            "Epoch 69/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23176.7797 - val_loss: 23792.0200\n",
            "Epoch 70/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23149.0122 - val_loss: 23914.0084\n",
            "Epoch 71/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23188.2631 - val_loss: 23728.5639\n",
            "Epoch 72/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 23098.3163 - val_loss: 23774.6300\n",
            "Epoch 73/100\n",
            "1400/1400 [==============================] - 317s 227ms/sample - loss: 23173.7278 - val_loss: 24024.1200\n",
            "Epoch 74/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23103.0921 - val_loss: 23997.0679\n",
            "Epoch 75/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23125.3823 - val_loss: 24017.8009\n",
            "Epoch 76/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23124.2613 - val_loss: 24051.8358\n",
            "Epoch 77/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 23097.4519 - val_loss: 23981.3398\n",
            "Epoch 78/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23142.2867 - val_loss: 23935.1282\n",
            "Epoch 79/100\n",
            "1400/1400 [==============================] - 314s 224ms/sample - loss: 23059.8434 - val_loss: 23828.2688\n",
            "Epoch 80/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23039.3112 - val_loss: 23724.1390\n",
            "Epoch 81/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23210.6153 - val_loss: 23982.1233\n",
            "Epoch 82/100\n",
            "1400/1400 [==============================] - 316s 225ms/sample - loss: 22962.3552 - val_loss: 23947.9369\n",
            "Epoch 83/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23094.5731 - val_loss: 23974.6650\n",
            "Epoch 84/100\n",
            "1400/1400 [==============================] - 317s 227ms/sample - loss: 23187.7524 - val_loss: 24095.3099\n",
            "Epoch 85/100\n",
            "1400/1400 [==============================] - 316s 225ms/sample - loss: 23122.7896 - val_loss: 23960.8966\n",
            "Epoch 86/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23048.1525 - val_loss: 23872.4253\n",
            "Epoch 87/100\n",
            "1400/1400 [==============================] - 317s 227ms/sample - loss: 23082.1973 - val_loss: 24062.8328\n",
            "Epoch 88/100\n",
            "1400/1400 [==============================] - 317s 227ms/sample - loss: 23024.7484 - val_loss: 24097.4929\n",
            "Epoch 89/100\n",
            "1400/1400 [==============================] - 317s 226ms/sample - loss: 23096.4137 - val_loss: 23928.6699\n",
            "Epoch 90/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23018.0477 - val_loss: 23813.9352\n",
            "Epoch 91/100\n",
            "1400/1400 [==============================] - 318s 227ms/sample - loss: 22913.8433 - val_loss: 23941.3872\n",
            "Epoch 92/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 22985.3165 - val_loss: 23901.4776\n",
            "Epoch 93/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23046.2201 - val_loss: 24142.6026\n",
            "Epoch 94/100\n",
            "1400/1400 [==============================] - 316s 225ms/sample - loss: 23019.4950 - val_loss: 24083.5900\n",
            "Epoch 95/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23034.2726 - val_loss: 24078.8254\n",
            "Epoch 96/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23026.8705 - val_loss: 23899.5317\n",
            "Epoch 97/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23012.8949 - val_loss: 24109.9635\n",
            "Epoch 98/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 22995.6054 - val_loss: 23797.5054\n",
            "Epoch 99/100\n",
            "1400/1400 [==============================] - 315s 225ms/sample - loss: 23132.3571 - val_loss: 25245.6937\n",
            "Epoch 100/100\n",
            "1400/1400 [==============================] - 316s 226ms/sample - loss: 23026.7382 - val_loss: 23891.5654\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc3eda36f10>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample one or more charachters and plot them\n",
        "random_chars = [np.random.normal(0, 1, latent_dim) for _ in range(1)]\n",
        "imgs = []\n",
        "for char in random_chars:\n",
        "    char = char.reshape(-1,2)\n",
        "    imgs.append(decoder.predict(char))\n",
        "\n",
        "imgs = [np.reshape(img,(image_width, image_height, 3)) for img in imgs]\n",
        "for img in imgs:\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "-sVrWC28jkyn",
        "outputId": "c679601e-7825-4bc1-ff53-01baf0b6f6df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e1703324f746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Randomly sample one or more charachters and plot them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_chars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e1703324f746>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Randomly sample one or more charachters and plot them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_chars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelling for Sketch To Face**"
      ],
      "metadata": {
        "id": "iXATrEy03wYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae.compile(optimizer='adam', loss=vae_loss)\n",
        "vae.fit(train_sketch_image, train_image,\n",
        "        epochs=200,\n",
        "        batch_size=16,\n",
        "        validation_data=(test_sketch_image, test_image))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p47sOEl30an",
        "outputId": "dd4f90d5-812f-416c-f650-acd48cd48163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1400 samples, validate on 104 samples\n",
            "Epoch 1/200\n",
            "1312/1400 [===========================>..] - ETA: 18s - loss: 46384.4808"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_chars = [np.random.normal(0, 1, latent_dim) for _ in range(1)]\n",
        "imgs = []\n",
        "for char in random_chars:\n",
        "    char = char.reshape(-1,2)\n",
        "    imgs.append(decoder.predict(char))\n",
        "\n",
        "imgs = [np.reshape(img,(image_width, image_height, 3)) for img in imgs]\n",
        "for img in imgs:\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "id": "9PGpEDXhHpCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KGruQj0yA9Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KR024gE5A9Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ckEMtBoWA9Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "4UlRjMX5A9kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(hp):\n",
        "  inputs = Input(shape=(image_width, image_height, 3), name='encoder_input')  \n",
        "\n",
        "  x = Conv2D(filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
        "             kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n",
        "             activation='relu',\n",
        "             strides=2, padding='same')(inputs)\n",
        "  \n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=hp.Int('conv_2_filter', min_value=64, max_value=128, step=16),\n",
        "             kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n",
        "             activation='relu',\n",
        "             strides=2, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  conv_shape = K.int_shape(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(16)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  mu = Dense(latent_dim, name='latent_mu')(x)\n",
        "  sigma = Dense(latent_dim, name='latent_sigma')(x)\n",
        "\n",
        "  # use reparameterization trick to push the sampling out as input\n",
        "  z  = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])\n",
        "\n",
        "\n",
        "  encoder = Model(inputs, [mu, sigma, z], name='encoder')\n",
        "  encoder.summary()\n",
        "  \n",
        "  # Defining decoder\n",
        "  d_i   = Input(shape=(latent_dim, ), name='decoder_input')\n",
        "  x     = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(d_i)\n",
        "  x     = BatchNormalization()(x)\n",
        "  x     = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
        "  cx    = Conv2DTranspose(16, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "  cx    = BatchNormalization()(cx)\n",
        "  cx    = Conv2DTranspose(8, (3, 3), strides=2, padding='same',  activation='relu')(cx)\n",
        "  cx    = BatchNormalization()(cx)\n",
        "  o     = Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same', name='decoder_output')(cx)\n",
        "\n",
        "  # Instantiate decoder\n",
        "  decoder = Model(d_i, o, name='decoder')\n",
        "  decoder.summary()\n",
        "\n",
        "  # Build VAE model\n",
        "  outputs = decoder(encoder(inputs)[2])\n",
        "  vae = Model(inputs, outputs, name='vae')\n",
        "  vae.summary()\n",
        "\n",
        "  vae.compile(optimizer='adam', loss=vae_loss)\n",
        "  return vae\n"
      ],
      "metadata": {
        "id": "7lD4VdM5BBKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_search=RandomSearch(build_model,\n",
        "                          objective='val_acc',\n",
        "                          max_trials=5,directory='/content/output3',project_name=\"FaceToSketch\")\n"
      ],
      "metadata": {
        "id": "a81JFG_tz5iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_search.search(train_sketch_image,train_image,epochs=500,validation_split=0.1, validation_data=(test_sketch_image, test_image))"
      ],
      "metadata": {
        "id": "XYU2wZIDzxJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tuner_search.get_best_models(num_models=1)[0]\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hlp8nRvf4Csl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_chars = [np.random.normal(0, 1, latent_dim) for _ in range(1)]\n",
        "imgs = []\n",
        "for char in random_chars:\n",
        "    char = char.reshape(-1,2)\n",
        "    imgs.append(model.predict(char))\n",
        "\n",
        "imgs = [np.reshape(img,(image_width, image_height, 3)) for img in imgs]\n",
        "for img in imgs:\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "id": "H77pf0R-4Ucf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}